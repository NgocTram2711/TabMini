import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.base import BaseEstimator, ClassifierMixin
import itertools

def get_device():
    return "cuda" if torch.cuda.is_available() else "cpu"

# Define SAINT model as per first code snippet
class SAINT(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes, lr=0.001, epochs=10, batch_size=32, dropout=0.2, path=None, time_limit=None, seed=None):
        super(SAINT, self).__init__()

        self.device = get_device()
        self.time_limit = time_limit
        self.path = path
        self.seed = seed
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        self.lr = lr
        self.epochs = epochs
        self.batch_size = batch_size
        self.dropout = dropout

        # Model architecture
        self.embedding = nn.Embedding(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.transformer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8)
        self.dropout_layer = nn.Dropout(p=self.dropout)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, X):
        x = self.embedding(X)
        x = self.relu(x)
        x = self.transformer(x)
        x = self.dropout_layer(x)
        x = x.mean(dim=1)
        x = self.fc(x)
        return x

    def fit(self, X, y):
        X_tensor = torch.tensor(X, dtype=torch.long)
        y_tensor = torch.tensor(y, dtype=torch.long)

        train_dataset = TensorDataset(X_tensor, y_tensor)
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)

        optimizer = optim.Adam(self.parameters(), lr=self.lr)
        criterion = nn.CrossEntropyLoss()

        self.train()
        for epoch in range(self.epochs):
            running_loss = 0.0
            for data, target in train_loader:
                optimizer.zero_grad()
                output = self(data)  # Forward pass
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                running_loss += loss.item()

            avg_loss = running_loss / len(train_loader)
            print(f"Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss:.4f}")

    def predict(self, X):
        X_tensor = torch.tensor(X, dtype=torch.long)
        self.eval()
        with torch.no_grad():
            output = self(X_tensor)
            _, predicted = torch.max(output, 1)
        return predicted.numpy()

    def score(self, X, y):
        y_pred = self.predict(X)
        return accuracy_score(y, y_pred)

    def save(self, filename):
        torch.save(self.state_dict(), filename)
        print(f'Model saved to {filename}')


class SAINT_Tuner(BaseEstimator, ClassifierMixin):
    def __init__(self, time_limit: int = 3600, device=None, seed: int = 42):
        self.time_limit = time_limit
        self.device = device if device else get_device()
        self.seed = seed
        self.best_params_ = None
        self.model = None

        self.param_grid = {
            "input_dim": [100, 150],
            "hidden_dim": [128, 256],
            "num_classes": [10, 5],
            "lr": [0.001, 0.0005],
            "epochs": [10, 20],
            "batch_size": [32, 64],
            "dropout": [0.2, 0.3]
        }

    def fit(self, X, y, X_test, y_test):
        # Split data into training and testing
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Create hyperparameter combinations
        param_combinations = [
            dict(zip(self.param_grid.keys(), v))
            for v in np.array(np.meshgrid(*self.param_grid.values())).T.reshape(-1, len(self.param_grid.keys()))
        ]

        results = []
        best_f1 = -1
        best_model = None
        
        for params in param_combinations:
            # Extract parameters
            input_dim = int(params["input_dim"])
            hidden_dim = int(params["hidden_dim"])
            num_classes = int(params["num_classes"])
            lr = float(params["lr"])
            epochs = int(params["epochs"])
            batch_size = int(params["batch_size"])
            dropout = float(params["dropout"])

            print(f"Training model with: {params}")
            model = SAINT(input_dim, hidden_dim, num_classes, lr, epochs, batch_size, dropout).to(self.device)
            model.fit(X_train, y_train)

            accuracy = model.score(X_test, y_test)
            f1 = f1_score(y_test, model.predict(X_test), average='weighted')

            results.append({**params, "accuracy": accuracy, "f1_score": f1})
            if f1 > best_f1:
                best_f1 = f1
                best_model = model
                self.best_params_ = params
        
        # Store results
        self.model = best_model
        self.result_df = pd.DataFrame(results).sort_values(by="f1_score", ascending=False)

    def predict(self, X):
        if self.model is None:
            raise RuntimeError("Model has not been fitted yet.")
        return self.model.predict(X)

    def save_results(self, filename):
        if self.result_df is not None:
            self.result_df.to_csv(filename, index=False)

# Example usage with Iris dataset
data = load_iris()
X, y = data.data, data.target

# Initialize and train the tuner
tuner = SAINT_Tuner(time_limit=3600)
tuner.fit(X, y, X, y)

# Save results to a CSV file
tuner.save_results('model_results.csv')
